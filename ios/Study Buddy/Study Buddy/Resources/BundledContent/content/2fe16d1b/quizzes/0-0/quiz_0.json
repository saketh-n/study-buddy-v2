{
  "topic_name": "Linear Algebra",
  "questions": [
    {
      "question": "In machine learning, why is the dot product between vectors particularly useful when computing similarity between data points?",
      "options": [
        "It measures the angle between vectors, with larger values indicating more similar directions",
        "It always produces values between 0 and 1, making it a natural probability measure",
        "It automatically normalizes the vectors to unit length",
        "It counts the number of matching features between two data points"
      ],
      "correct_index": 0,
      "explanation": "The dot product captures both magnitude and angular similarity between vectors. When vectors point in similar directions, their dot product is large and positive, making it useful for measuring similarity in high-dimensional feature spaces."
    },
    {
      "question": "A data scientist has a dataset with 1000 samples and 50 features. After applying Principal Component Analysis (PCA), why might they choose to keep only the first 10 principal components?",
      "options": [
        "The first 10 components always contain exactly 80% of the original information",
        "The first 10 eigenvalues are positive while the rest are negative",
        "The first 10 components capture most of the variance while reducing computational complexity",
        "PCA requires keeping exactly 20% of the original dimensions"
      ],
      "correct_index": 2,
      "explanation": "PCA orders components by the amount of variance they explain (eigenvalues). Keeping the first few components retains most important information while dramatically reducing dimensionality, leading to faster computation and reduced overfitting."
    },
    {
      "question": "When training a neural network, the weight matrix W has dimensions (128, 64) and the input vector x has dimensions (128, 1). What does the resulting matrix multiplication Wx represent?",
      "options": [
        "128 different linear combinations of the 64 input features",
        "64 different linear combinations of the 128 input features",
        "A single scalar value representing the network's confidence",
        "An error that will crash the program due to dimension mismatch"
      ],
      "correct_index": 1,
      "explanation": "Matrix multiplication Wx with W being (128,64) and x being (128,1) produces a (64,1) vector. Each of the 64 output values is a weighted sum (linear combination) of all 128 input features, representing how each neuron in the next layer responds to the input."
    },
    {
      "question": "In the context of solving the normal equation for linear regression (X^T X)\u03b2 = X^T y, when would the matrix X^T X become non-invertible, and what does this indicate about your data?",
      "options": [
        "When you have more features than training samples, indicating potential overfitting",
        "When your features are perfectly linearly dependent, making the solution non-unique",
        "When the target variable y has zero variance across all samples",
        "When the learning rate is set too high during gradient descent"
      ],
      "correct_index": 1,
      "explanation": "X^T X becomes singular (non-invertible) when columns of X are linearly dependent, meaning some features can be expressed as combinations of others. This creates infinite possible solutions since the same predictions can be achieved with different parameter values."
    },
    {
      "question": "A recommendation system represents users and items as vectors in the same embedding space. If user vector u = [2, -1, 3] and item vector i = [1, 2, 1], and you compute u^T i = 3, what can you conclude about this user-item relationship?",
      "options": [
        "The user will definitely like this item since the dot product is positive",
        "The user and item vectors are orthogonal, indicating no relationship",
        "This represents a weak positive relationship, but other items might be better matches",
        "The similarity score is too low to make any meaningful recommendation"
      ],
      "correct_index": 2,
      "explanation": "The dot product of 3 indicates a positive but modest relationship. In recommendation systems, you typically want to recommend items with the highest dot products for a given user, so while this is positive, comparing with other items would determine if this is actually a good recommendation."
    }
  ],
  "passing_score": 80
}